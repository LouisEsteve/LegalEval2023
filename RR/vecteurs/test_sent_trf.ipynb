{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from string import punctuation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\jokkl\\\\Documents\\\\Masters\\\\S3\\\\Projet en TAL\\\\Shared\\\\RR\\\\Data'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR = Path(Path.cwd()).parent\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"Data\")\n",
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(Path.cwd()).parent\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"Data\")\n",
    "files = [file for file in os.listdir(DATA_DIR) if \".csv\" in file]\n",
    "\n",
    "for file in files: \n",
    "    if file==\"train.csv\":\n",
    "        TRAIN = os.path.join(DATA_DIR, file)\n",
    "    elif file==\"dev.csv\":\n",
    "        DEV = os.path.join(DATA_DIR, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RR_DEV_alt.csv', 'RR_TRAIN_alt.csv']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jokkl\\miniconda3\\envs\\projetTAL\\lib\\site-packages\\spacy\\util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.2.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.4.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Traiter/Nettoyer un texte pour récupérer les lemmes, en enlevant les stopwords et les termes de ponctuation.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        ents = [ent.text for ent in doc.ents]\n",
    "        if token.lemma_ not in set(stopwords) and token.lemma_ not in punctuation:\n",
    "            if token.text not in ents:\n",
    "                lemmas.append(token.lemma_)\n",
    "    processed = ' '.join(token for token in lemmas)\n",
    "    return processed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TRAIN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jokkl\\Documents\\Masters\\S3\\Projet en TAL\\Shared\\RR\\vecteurs\\test_sent_trf.ipynb Cellule 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jokkl/Documents/Masters/S3/Projet%20en%20TAL/Shared/RR/vecteurs/test_sent_trf.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(TRAIN, sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m;\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUTF-8\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TRAIN' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(TRAIN, sep=\";\", encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatiser les textes d'entrée\n",
    "print(\"Lemmatising...\\n\")\n",
    "df[\"lemmatised\"] = df[\"text\"].progress_apply(process_text)\n",
    "print(\"Lemmatisation completed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projetTAL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f6e0642978f65ae5477622b56b60b84182cf7a3c5ad36a67da23cabaa2e282b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
